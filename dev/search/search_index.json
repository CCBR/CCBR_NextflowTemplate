{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Background","text":""},{"location":"#nextflow-template","title":"Nextflow Template","text":"<p>CCBR template for creating Nextflow pipelines </p> <p> </p> <p>See the website for detailed information, documentation, and examples: https://ccbr.github.io/TOOL_NAME/</p>"},{"location":"#using-this-template","title":"Using this template","text":"<ol> <li>Create a new repository from this template using either of these options:</li> <li>The GitHub web interface:      Click \"Use this template\" and \"Create a new repository\", then choose an owner (e.g. CCBR or another organization) and the repository name as the new tool's name.</li> <li>The GitHub command line interface:      Replace <code>OWNER/TOOL_NAME</code> with your organization (e.g. CCBR) and the actual tool name.      <pre><code>gh repo create OWNER/TOOL_NAME \\\n--description \"One-line description of your tool\" \\\n--public \\\n--template CCBR/CCBR_NextflowTemplate \\\n--confirm\n</code></pre></li> <li>Read and follow the contributing guidelines in <code>.github/CONTRIBUTING.md</code>.    Be sure to install <code>pre-commit</code> and its hooks before making any commits.</li> <li>Change all instances of <code>TOOL_NAME</code> and <code>tool_name</code> throughout the repo with the actual tool name.    Replace <code>TOOL_NAME</code> with the all-caps version (refers to the GitHub repo name)    and <code>tool_name</code> with the lowercase version (refers to the command-line interface). Places include:</li> </ol> <pre><code>.github/CONTRIBUTING.md\n.github/ISSUE_TEMPLATE/bug_report.yml\n.github/ISSUE_TEMPLATE/config.yml\n.github/workflows/build.yml\nbin/tool_name\nCHANGELOG.md\nCITATION.cff\nREADME.md\nmain.nf\nmkdocs.yml\nnextflow.config\npyproject.toml\nsrc/__main__.py\n</code></pre> <ol> <li>Edit <code>pyproject.toml</code> and <code>nextflow.config</code> with correct metadata for your tool. You will likely need to change:</li> <li>author names and emails</li> <li>dependencies</li> <li>project URLs</li> <li>Write your nextflow workflow.</li> <li>Where possible, reuse existing modules and subworklows from CCBR/nf-modules3.      Also consider contributing new modules &amp; subworkflows to that repository!</li> <li>Write your documentation in <code>docs/</code> and enable GitHub Pages.</li> <li>In settings, go to General &gt; Pages and select the <code>gh-pages</code> branch.      mkdocs will build your site under the <code>gh-pages</code> branch, and GitHub Pages will make it available at <code>https://OWNER.github.io/TOOL_NAME</code>.</li> <li>Edit the README:</li> <li>Change the title and description.</li> <li>Delete the section Using this template.</li> <li>You can look for instances of <code>TOOL_NAME</code> in case you missed any with grep:</li> </ol> <pre><code>grep -ir \"TOOL_NAME\" .\n</code></pre> <p>If your repo is not part of CCBR, you will also want to look for instances of \"CCBR\" and \"CCR Collaborative Bioinformatics Resource\" and replace them with your organization.</p> <pre><code>grep -ir \"CCBR\\|CCR\" .\n</code></pre> <p>For a work-in-progress example of this template in action, see the CHAMPAGNE repo.</p>"},{"location":"#usage","title":"Usage","text":"<p>Install the tool in edit mode:</p> <pre><code>pip3 install -e .\n</code></pre> <p>View CLI options:</p> <pre><code>tool_name --help\n</code></pre> <p>Navigate to your project directory and initialize required config files:</p> <pre><code>tool_name init\n</code></pre> <p>Run the example</p> <pre><code>tool_name run --input \"Hello world\"\n</code></pre> <p></p>"},{"location":"#help-contributing","title":"Help &amp; Contributing","text":"<p>Come across a bug? Open an issue and include a minimal reproducible example.</p> <p>Have a question? Ask it in discussions.</p> <p>Want to contribute to this project? Check out the contributing guidelines.</p>"},{"location":"#references","title":"References","text":"<p>This repo was originally generated from the CCBR Nextflow Template. The template takes inspiration from nektool1 and the nf-core template. If you plan to contribute your pipeline to nf-core, don't use this template -- instead follow nf-core's instructions2.</p> <ol> <li> <p>nektool https://github.com/beardymcjohnface/nektool \u21a9</p> </li> <li> <p>instructions for nf-core pipelines https://nf-co.re/docs/contributing/tutorials/creating_with_nf_core \u21a9</p> </li> <li> <p>See also our reusable modules and subworkflows for CCBR nextflow pipelines: https://github.com/CCBR/nf-modules \u21a9</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#tool_name-development-version","title":"TOOL_NAME development version","text":""},{"location":"changelog/#api-breaking-changes","title":"API-breaking changes","text":"<ul> <li>change 1</li> <li>example 2</li> </ul>"},{"location":"changelog/#new-features","title":"New features","text":"<ul> <li>new feat 1</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug fixes","text":"<ul> <li>description of bug fixed</li> </ul>"},{"location":"changelog/#tool_name-v010","title":"TOOL_NAME v0.1.0","text":"<p>This is the first release of TOOL_NAME \ud83c\udf89</p>"},{"location":"contributing/","title":"Contributing to TOOL_NAME","text":""},{"location":"contributing/#proposing-changes-with-issues","title":"Proposing changes with issues","text":"<p>If you want to make a change, it's a good idea to first open an issue and make sure someone from the team agrees that it\u2019s needed.</p> <p>If you've decided to work on an issue, assign yourself to the issue so others will know you're working on it.</p>"},{"location":"contributing/#pull-request-process","title":"Pull request process","text":"<p>We use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to TOOL_NAME.</p> <p></p>"},{"location":"contributing/#clone-the-repo","title":"Clone the repo","text":"<p>If you are a member of CCBR, you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once.</p> <pre><code>git clone https://github.com/CCBR/TOOL_NAME\n</code></pre> <p>Cloning into 'TOOL_NAME'...  remote: Enumerating objects: 1136, done.  remote: Counting objects: 100% (463/463), done.  remote: Compressing objects: 100% (357/357), done.  remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673  Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done.  Resolving deltas: 100% (530/530), done. </p> <pre><code>cd TOOL_NAME\n</code></pre>"},{"location":"contributing/#if-this-is-your-first-time-cloning-the-repo-you-may-need-to-install-dependencies","title":"If this is your first time cloning the repo, you may need to install dependencies","text":"<ul> <li> <p>Install nextflow and singularity or docker if needed (biowulf already has these available as modules).</p> </li> <li> <p>Install the python dependencies with pip</p> </li> </ul> <pre><code>pip install .\n</code></pre> <p>If you're developing on biowulf, you can use our shared conda environment which already has these dependencies installed</p> <pre><code>. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\nconda activate py311\n</code></pre> <ul> <li>Install <code>pre-commit</code> if you don't already   have it. Then from the repo's root directory, run</li> </ul> <pre><code>pre-commit install\n</code></pre> <p>This will install the repo's pre-commit hooks.   You'll only need to do this step the first time you clone the repo.</p>"},{"location":"contributing/#create-a-branch","title":"Create a branch","text":"<p>Create a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as <code>iss-10</code> if it is for a specific issue.</p> <pre><code># create a new branch and switch to it\ngit branch iss-10\ngit switch iss-10\n</code></pre> <p>Switched to a new branch 'iss-10'</p>"},{"location":"contributing/#make-your-changes","title":"Make your changes","text":"<p>Edit the code, write and run tests, and update the documentation as needed.</p>"},{"location":"contributing/#test","title":"test","text":"<p>Changes to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the <code>tests/</code> subdirectory. Run the tests with <code>python -m pytest</code>.</p> <p>If you change the workflow, please run the workflow with the test profile and make sure your new feature or bug fix works as intended.</p>"},{"location":"contributing/#document","title":"document","text":"<p>If you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in <code>docs/</code>.</p>"},{"location":"contributing/#commit-and-push-your-changes","title":"Commit and push your changes","text":"<p>If you're not sure how often you should commit or what your commits should consist of, we recommend following the \"atomic commits\" principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/</p> <p>First, add the files that you changed to the staging area:</p> <pre><code>git add path/to/changed/files/\n</code></pre> <p>Then make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as <code>feat</code>, <code>fix</code>, <code>docs</code>, etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages.</p> <pre><code>git commit -m 'feat: create function for awesome feature'\n</code></pre> <p>pre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed.</p> <p>Check for added large files..............................................Passed  Fix End of Files.........................................................Passed  Trim Trailing Whitespace.................................................Failed </p> <ul> <li>hook id: trailing-whitespace </li> <li>exit code: 1 </li> <li>files were modified by this hook  &gt;    Fixing path/to/changed/files/file.txt  &gt;    codespell................................................................Passed    style-files..........................................(no files to check)Skipped    readme-rmd-rendered..................................(no files to check)Skipped    use-tidy-description.................................(no files to check)Skipped </li> </ul> <p>In the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run <code>git diff</code> to see the changes that pre-commit made and <code>git status</code> to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command:</p> <pre><code>git add path/to/changed/files/file.txt\ngit commit -m 'feat: create function for awesome feature'\n</code></pre> <p>This time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created.</p> <p>Check for added large files..............................................Passed  Fix End of Files.........................................................Passed  Trim Trailing Whitespace.................................................Passed  codespell................................................................Passed  style-files..........................................(no files to check)Skipped  readme-rmd-rendered..................................(no files to check)Skipped  use-tidy-description.................................(no files to check)Skipped  Conventional Commit......................................................Passed  &gt; [iss-10 9ff256e] feat: create function for awesome feature  1 file changed, 22 insertions(+), 3 deletions(-) </p> <p>Finally, push your changes to GitHub:</p> <pre><code>git push\n</code></pre> <p>If this is the first time you are pushing this branch, you may have to explicitly set the upstream branch:</p> <pre><code>git push --set-upstream origin iss-10\n</code></pre> <p>Enumerating objects: 7, done.  Counting objects: 100% (7/7), done.  Delta compression using up to 10 threads  Compressing objects: 100% (4/4), done.  Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done.  Total 4 (delta 3), reused 0 (delta 0), pack-reused 0  remote: Resolving deltas: 100% (3/3), completed with 3 local objects.  remote:  remote: Create a pull request for 'iss-10' on GitHub by visiting:  remote: https://github.com/CCBR/TOOL_NAME/pull/new/iss-10  remote:  To https://github.com/CCBR/TOOL_NAME  &gt;  &gt; [new branch] iss-10 -&gt; iss-10  branch 'iss-10' set up to track 'origin/iss-10'. </p> <p>We recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at <code>https://github.com/CCBR/TOOL_NAME/tree/&lt;your-branch-name&gt;</code> (replace <code>&lt;your-branch-name&gt;</code> with the actual name of your branch).</p>"},{"location":"contributing/#create-the-pr","title":"Create the PR","text":"<p>Once your branch is ready, create a PR on GitHub: https://github.com/CCBR/TOOL_NAME/pull/new/</p> <p>Select the branch you just pushed:</p> <p></p> <p>Edit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between <code>&lt;!--</code> and <code>--&gt;</code>) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you're ready, click 'Create pull request' to open it.</p> <p></p> <p>Optionally, you can mark the PR as a draft if you're not yet ready for it to be reviewed, then change it later when you're ready.</p>"},{"location":"contributing/#wait-for-a-maintainer-to-review-your-pr","title":"Wait for a maintainer to review your PR","text":"<p>We will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/. The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that's the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR.</p> <p>Once the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution!</p>"},{"location":"contributing/#after-your-pr-has-been-merged","title":"After your PR has been merged","text":"<p>After your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes:</p> <pre><code>git checkout main\ngit pull\n</code></pre> <p>It's a good idea to run <code>git pull</code> before creating a new branch so it will start from the most recent commits in main.</p>"},{"location":"contributing/#helpful-links-for-more-information","title":"Helpful links for more information","text":"<ul> <li>GitHub Flow</li> <li>semantic versioning guidelines</li> <li>changelog guidelines</li> <li>tidyverse code review principles</li> <li>reproducible examples</li> <li>nf-core extensions for VS Code</li> </ul>"},{"location":"contributors/","title":"Contributors","text":"<p>Should include a list of all contributors, including GitHub handles when appropriate. In addition, a statement of who contributed to the source code specifically, identified by initials. An example is included below.</p> <p>TODO: populate this automagically similar to https://nf-co.re/contributors? or link to GitHub contributor page? could use gh action: https://github.com/lowlighter/metrics/blob/master/source/plugins/contributors/README.md</p>"},{"location":"contributors/#contributions","title":"Contributions","text":"<p>The following members contributed to the development of the CARLISLE pipeline:</p> <ul> <li>Samantha Sevilla</li> </ul> <p>SS contributed to the generating the source code and all members contributed to the main concepts and analysis.</p>"},{"location":"user-guide/getting-started/","title":"1. Getting Started","text":"<p>This should set the stage for all of the pipeline requirements. Examples are listed below.</p>"},{"location":"user-guide/getting-started/#overview","title":"Overview","text":"<p>The CARLISLE github repository is stored locally, and will be used for project deployment. Multiple projects can be deployed from this one point simultaneously, without concern.</p>"},{"location":"user-guide/getting-started/#1-getting-started","title":"1. Getting Started","text":""},{"location":"user-guide/getting-started/#11-introduction","title":"1.1 Introduction","text":"<p>The CARLISLE Pipelie beings with raw FASTQ files and performs trimming followed by alignment using BOWTIE2. Data is then normalized through either the use of an user-species species (IE E.Coli) spike-in control or through the determined library size. Peaks are then called using MACS2, SEACR, and GoPEAKS with various options selected by the user. Peaks are then annotated, and summarized into reports. If designated, differential analysis is performed using DESEQ2. QC reports are also generated with each project using FASTQC and MULTIQC. Annotations are added using HOMER and ROSE. GSEA Enrichment analysis predictions are added using CHIPENRICH.</p> <p>The following are sub-commands used within CARLISLE:</p> <ul> <li>initialize: initialize the pipeline</li> <li>dryrun: predict the binding of peptides to any MHC molecule</li> <li>cluster: execute the pipeline on the Biowulf HPC</li> <li>local: execute a local, interactive, session</li> <li>git: execute GitHub actions</li> <li>unlock: unlock directory</li> <li>DAG: create DAG report</li> <li>report: create SNAKEMAKE report</li> <li>testrun: copies test manifests and files to WORKDIR</li> </ul>"},{"location":"user-guide/getting-started/#12-setup-dependencies","title":"1.2 Setup Dependencies","text":"<p>CARLISLE has several dependencies listed below. These dependencies can be installed by a sysadmin. All dependencies will be automatically loaded if running from Biowulf.</p> <ul> <li>bedtools: \"bedtools/2.30.0\"</li> <li>bedops: \"bedops/2.4.40\"</li> </ul>"},{"location":"user-guide/getting-started/#13-login-to-the-cluster","title":"1.3 Login to the cluster","text":"<p>CARLISLE has been exclusively tested on Biowulf HPC. Login to the cluster's head node and move into the pipeline location.</p> <pre><code># ssh into cluster's head node\nssh -Y $USER@biowulf.nih.gov\n</code></pre>"},{"location":"user-guide/getting-started/#14-load-an-interactive-session","title":"1.4 Load an interactive session","text":"<p>An interactive session should be started before performing any of the pipeline sub-commands, even if the pipeline is to be executed on the cluster.</p> <pre><code># Grab an interactive node\nsinteractive --time=12:00:00 --mem=8gb  --cpus-per-task=4 --pty bash\n</code></pre>"},{"location":"user-guide/output/","title":"4. Expected Output","text":"<p>This should include all pertitant information about output files, including extensions that differentiate files. An example is provided below.</p>"},{"location":"user-guide/output/#4-expected-outputs","title":"4. Expected Outputs","text":"<p>The following directories are created under the WORKDIR/results directory:</p> <ul> <li>alignment_stats: this directory include information on the alignment of each sample</li> <li>peaks: this directory contains a sub-directory that relates to the quality threshold used.</li> <li>quality threshold<ul> <li>contrasts: this directory includes the contrasts for each line listed in the contrast manifest</li> <li>peak_caller: this directory includes all peak calls from each peak_caller (SEACR, MACS2, GOPEAKS) for each sample</li> <li>annotation<ul> <li>go_enrichment: this directory includes gene set enrichment pathway predictions</li> <li>homer: this directory includes the annotation output from HOMER</li> <li>rose: this directory includes the annotation output from ROSE</li> </ul> </li> </ul> </li> </ul> <pre><code>\u251c\u2500\u2500 alignment_stats\n\u251c\u2500\u2500 bam\n\u251c\u2500\u2500 peaks\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 0.05\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrasts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrast_id1.dedup_status\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 contrast_id2.dedup_status\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 gopeaks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 annotation\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 go_enrichment\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrast_id1.dedup_status.go_enrichment_tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 contrast_id2.dedup_status.go_enrichment_html_report\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homer\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.gopeaks_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.gopeaks_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rose\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.dedup.gopeaks_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.dedup.gopeaks_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 peak_output\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 macs2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 annotation\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 go_enrichment\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrast_id1.dedup_status.go_enrichment_tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 contrast_id2.dedup_status.go_enrichment_html_report\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homer\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rose\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 peak_output\n</code></pre>"},{"location":"user-guide/preparing-files/","title":"2. Preparing Files","text":"<p>This should describe any input files needed, including config files, manifest files, and sample files. An example is provided below.</p>"},{"location":"user-guide/preparing-files/#2-preparing-files","title":"2. Preparing Files","text":"<p>The pipeline is controlled through editing configuration and manifest files. Defaults are found in the /WORKDIR/config and /WORKDIR/manifest directories, after initialization.</p>"},{"location":"user-guide/preparing-files/#21-configs","title":"2.1 Configs","text":"<p>The configuration files control parameters and software of the pipeline. These files are listed below:</p> <ul> <li>config/config.yaml</li> <li>resources/cluster.yaml</li> <li>resources/tools.yaml</li> </ul>"},{"location":"user-guide/preparing-files/#211-cluster-config","title":"2.1.1 Cluster Config","text":"<p>The cluster configuration file dictates the resources to be used during submission to Biowulf HPC. There are two different ways to control these parameters - first, to control the default settings, and second, to create or edit individual rules. These parameters should be edited with caution, after significant testing.</p>"},{"location":"user-guide/preparing-files/#212-tools-config","title":"2.1.2 Tools Config","text":"<p>The tools configuration file dictates the version of each software or program that is being used in the pipeline.</p>"},{"location":"user-guide/preparing-files/#213-config-yaml","title":"2.1.3 Config YAML","text":"<p>There are several groups of parameters that are editable for the user to control the various aspects of the pipeline. These are :</p> <ul> <li>Folders and Paths</li> <li>These parameters will include the input and output files of the pipeline, as well as list all manifest names.</li> <li>User parameters</li> <li>These parameters will control the pipeline features. These include thresholds and whether to perform processes.</li> <li>References</li> <li>These parameters will control the location of index files, spike-in references, adaptors and species calling information.</li> </ul>"},{"location":"user-guide/preparing-files/#2131-user-parameters","title":"2.1.3.1 User Parameters","text":""},{"location":"user-guide/preparing-files/#21311-duplication-status","title":"2.1.3.1.1 Duplication Status","text":"<p>Users can select duplicated peaks (dedup) or non-deduplicated peaks (no_dedup) through the user parameter.</p> <pre><code>dupstatus: \"dedup, no_dedup\"\n</code></pre>"},{"location":"user-guide/preparing-files/#21312-macs2-additional-option","title":"2.1.3.1.2 Macs2 additional option","text":"<p>MACS2 can be run with or without the control. adding a control will increase peak specificity Selecting \"Y\" for the <code>macs2_control</code> will run the paired control sample provided in the sample manifest</p>"},{"location":"user-guide/preparing-files/#2132-references","title":"2.1.3.2 References","text":"<p>Additional reference files may be added to the pipeline, if other species were to be used.</p> <p>The absolute file paths which must be included are:</p> <ol> <li>fa: \"/path/to/species.fa\"</li> <li>blacklist: \"/path/to/blacklistbed/species.bed\"</li> </ol> <p>The following information must be included:</p> <ol> <li>regions: \"list of regions to be included; IE chr1 chr2 chr3\"</li> <li>macs2_g: \"macs2 genome shorthand; IE mm IE hs\"</li> </ol>"},{"location":"user-guide/preparing-files/#22-preparing-manifests","title":"2.2 Preparing Manifests","text":"<p>There are two manifests, one which required for all pipelines and one that is only required if running a differential analysis. These files describe information on the samples and desired contrasts. The paths of these files are defined in the snakemake_config.yaml file. These files are:</p> <ul> <li>samplemanifest</li> <li>contrasts</li> </ul>"},{"location":"user-guide/preparing-files/#221-samples-manifest-required","title":"2.2.1 Samples Manifest (REQUIRED)","text":"<p>This manifest will include information to sample level information. It includes the following column headers:</p> <ul> <li>sampleName: the sample name WITHOUT replicate number (IE \"SAMPLE\")</li> <li>replicateNumber: the sample replicate number (IE \"1\")</li> <li>isControl: whether the sample should be identified as a control (IE \"Y\")</li> <li>controlName: the name of the control to use for this sample (IE \"CONTROL\")</li> <li>controlReplicateNumber: the replicate number of the control to use for this sample (IE \"1\")</li> <li>path_to_R1: the full path to R1 fastq file (IE \"/path/to/sample1.R1.fastq\")</li> <li>path_to_R2: the full path to R1 fastq file (IE \"/path/to/sample2.R2.fastq\")</li> </ul> <p>An example sampleManifest file is shown below:</p> sampleName replicateNumber isControl controlName controlReplicateNumber path_to_R1 path_to_R2 53_H3K4me3 1 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/53_H3K4me3_1.R1.fastq.gz PIPELINE_HOME/.test/53_H3K4me3_1.R2.fastq.gz 53_H3K4me3 2 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/53_H3K4me3_2.R1.fastq.gz PIPELINE_HOME/.test/53_H3K4me3_2.R2.fastq.gz HN6_H3K4me3 1 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/HN6_H3K4me3_1.R1.fastq.gz PIPELINE_HOME/.test/HN6_H3K4me3_1.R2.fastq.gz HN6_H3K4me3 2 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/HN6_H3K4me3_2.R1.fastq.gz PIPELINE_HOME/.test/HN6_H3K4me3_2.R2.fastq.gz HN6_IgG_rabbit_negative_control 1 Y - - PIPELINE_HOME/.test/HN6_IgG_rabbit_negative_control_1.R1.fastq.gz PIPELINE_HOME/.test/HN6_IgG_rabbit_negative_control_1.R2.fastq.gz"},{"location":"user-guide/run/","title":"3. Running the Pipeline","text":"<p>This should include all information about the various run commands provided within the pipeline.</p>"},{"location":"user-guide/run/#3-running-the-pipeline","title":"3. Running the Pipeline","text":""},{"location":"user-guide/run/#31-pipeline-overview","title":"3.1 Pipeline Overview","text":"<p>The Snakemake workflow has a multiple options:</p> <pre><code>Usage: bash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle -m/--runmode=&lt;RUNMODE&gt; -w/--workdir=&lt;WORKDIR&gt;\n1.  RUNMODE: [Type: String] Valid options:\n    *) init : initialize workdir\n    *) run : run with slurm\n    *) reset : DELETE workdir dir and re-init it\n    *) dryrun : dry run snakemake to generate DAG\n    *) unlock : unlock workdir if locked by snakemake\n    *) runlocal : run without submitting to sbatch\n    *) testrun: run on cluster with included test dataset\n2.  WORKDIR: [Type: String]: Absolute or relative path to the output folder with write permissions.\n</code></pre>"},{"location":"user-guide/run/#32-commands-explained","title":"3.2 Commands explained","text":"<p>The following explains each of the command options:</p> <ul> <li>Preparation Commands</li> <li>init (REQUIRED): This must be performed before any Snakemake run (dry, local, cluster) can be performed. This will copy the necessary config, manifest and Snakefiles needed to run the pipeline to the provided output directory.</li> <li>dryrun (OPTIONAL): This is an optional step, to be performed before any Snakemake run (local, cluster). This will check for errors within the pipeline, and ensure that you have read/write access to the files needed to run the full pipeline.</li> <li>Processing Commands</li> <li>local: This will run the pipeline on a local node. NOTE: This should only be performed on an interactive node.</li> <li>run: This will submit a master job to the cluster, and subsequent sub-jobs as needed to complete the workflow. An email will be sent when the pipeline begins, if there are any errors, and when it completes.</li> <li>Other Commands (All optional)</li> <li>unlock: This will unlock the pipeline if an error caused it to stop in the middle of a run.</li> <li>testrun: This will run a test of the pipeline with test data</li> </ul> <p>To run any of these commands, follow the the syntax:</p> <pre><code>bash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=COMMAND --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/run/#33-typical-workflow","title":"3.3 Typical Workflow","text":"<p>A typical command workflow, running on the cluster, is as follows:</p> <pre><code>bash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=init --workdir=/path/to/output/dir\n\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=dryrun --workdir=/path/to/output/dir\n\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=run --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/test-info/","title":"5. Running Test Data","text":"<p>This should walk the user through the steps of running the pipeline using test data</p>"},{"location":"user-guide/test-info/#5-pipeline-tutorial","title":"5. Pipeline Tutorial","text":"<p>Welcome to the CARLISLE Pipeline Tutorial!</p>"},{"location":"user-guide/test-info/#51-getting-started","title":"5.1 Getting Started","text":"<p>Review the information on the Getting Started for a complete overview the pipeline. The tutorial below will use test data available on NIH Biowulf HPC only. All example code will assume you are running v1.0 of the pipeline, using test data available on GitHub.</p> <p>A. Change working directory to the CARLISLE repository</p> <p>B. Initialize Pipeline</p> <pre><code>bash ./path/to/dir/carlisle --runmode=init --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/test-info/#52-about-the-test-data","title":"5.2 About the test data","text":"<p>This test data consists of sub-sampled inputs, consisting of two pairs of two replicate samples and one control. The reference to be used is hg38.</p>"},{"location":"user-guide/test-info/#53-submit-the-test-data","title":"5.3 Submit the test data","text":"<p>Test data is included in the .test directory as well as the config directory.</p> <p>A Run the test command to prepare the data, perform a dry-run and submit to the cluster</p> <pre><code>bash ./path/to/dir/carlisle --runmode=testrun --workdir=/path/to/output/dir\n</code></pre> <ul> <li>An expected output for the <code>testrun</code> is as follows:</li> </ul> <pre><code>Job stats:\njob                              count    min threads    max threads\n-----------------------------  -------  -------------  -------------\nDESeq                                  24              1              1\nalign                                   9             56             56\nalignstats                              9              2              2\nall                                     1              1              1\nbam2bg                                  9              4              4\ncreate_contrast_data_files             24              1              1\ncreate_contrast_peakcaller_files       12              1              1\ncreate_reference                        1             32             32\ncreate_replicate_sample_table           1              1              1\ndiffbb                                 24              1              1\nfilter                                 18              2              2\nfindMotif                              96              6              6\ngather_alignstats                       1              1              1\ngo_enrichment                          12              1              1\ngopeaks_broad                          16              2              2\ngopeaks_narrow                         16              2              2\nmacs2_broad                            16              2              2\nmacs2_narrow                           16              2              2\nmake_counts_matrix                     24              1              1\nmultiqc                                 2              1              1\nqc_fastqc                               9              1              1\nrose                                   96              2              2\nseacr_relaxed                          16              2              2\nseacr_stringent                        16              2              2\nspikein_assessment                      1              1              1\ntrim                                    9             56             56\ntotal                                 478              1             56\n</code></pre>"},{"location":"user-guide/test-info/#54-review-outputs","title":"5.4 Review outputs","text":"<p>Review the expected outputs on the Output page. If there are errors, review and performing stesp described on the Troubleshooting page as needed.</p>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"<p>This should include basic information on how to troubleshoot the pipeline. It should also include the main pipeliner developers contact information for users to utilize, as needed.</p>"},{"location":"user-guide/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Recommended steps to troubleshoot the pipeline.</p>"},{"location":"user-guide/troubleshooting/#11-email","title":"1.1 Email","text":"<p>Check your email for an email regarding pipeline failure. You will receive an email from slurm@biowulf.nih.gov with the subject: Slurm Job_id=[#] Name=CARLISLE Failed, Run time [time], FAILED, ExitCode 1</p>"},{"location":"user-guide/troubleshooting/#12-review-the-log-files","title":"1.2 Review the log files","text":"<p>Review the logs in two ways:</p> <ol> <li>Review the master slurm file: This file will be found in the <code>/path/to/results/dir/</code> and titled <code>slurm-[jobid].out</code>. Reviewing this file will tell you what rule errored, and for any local SLURM jobs, provide error details</li> <li>Review the individual rule log files: After reviewing the master slurm-file, review the specific rules that failed within the <code>/path/to/results/dir/logs/</code>. Each rule will include a <code>.err</code> and <code>.out</code> file, with the following formatting: <code>{rulename}.{masterjobID}.{individualruleID}.{wildcards from the rule}.{out or err}</code></li> </ol>"},{"location":"user-guide/troubleshooting/#13-restart-the-run","title":"1.3 Restart the run","text":"<p>After addressing the issue, unlock the output directory, perform another dry-run and check the status of the pipeline, then resubmit to the cluster.</p> <pre><code>#unlock dir\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=unlock --workdir=/path/to/output/dir\n\n#perform dry-run\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=dryrun --workdir=/path/to/output/dir\n\n#submit to cluster\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=run --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/troubleshooting/#14-contact-information","title":"1.4 Contact information","text":"<p>If after troubleshooting, the error cannot be resolved, or if a bug is found, please create an issue and send and email to Samantha Chill.</p>"}]}